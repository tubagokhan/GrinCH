{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhHuIYSe6fRVTLxMx/gjVy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tubagokhan/GrinCH/blob/main/RedundancyMeasureRL_ReScore_NGram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alzKWpHFJhU8",
        "outputId": "b7c5f8fe-9cfb-44e8-cdfa-47354e01a42c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "sys.setrecursionlimit(10000000)\n",
        "print(sys.getrecursionlimit())\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install rouge\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RL-Redundancy\n",
        "\n",
        "from rouge import Rouge\n",
        "from itertools import combinations\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "def remove_short_strings(lst, min_length=2):\n",
        "    return [item for item in lst if len(item) >= min_length]\n",
        "\n",
        "def rouge_l_f1_score(sentence1, sentence2):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(sentence1, sentence2)\n",
        "    rouge_l = scores[0]['rouge-l']['f']\n",
        "    return rouge_l\n",
        "\n",
        "\n",
        "def calculate_redundancy_Rouge_score(sentences):\n",
        "    if len(sentences) < 2:\n",
        "        return 0\n",
        "\n",
        "    summary_sentences = remove_short_strings(sentences)\n",
        "    total_redundancy_score = 0\n",
        "    num_pairs = 0\n",
        "\n",
        "    for sentence1, sentence2 in combinations(summary_sentences, 2):\n",
        "        rouge_l_score = rouge_l_f1_score(sentence1, sentence2)\n",
        "        total_redundancy_score += rouge_l_score\n",
        "        num_pairs += 1\n",
        "\n",
        "    if num_pairs > 0:\n",
        "        average_redundancy_score = total_redundancy_score / num_pairs\n",
        "        return average_redundancy_score\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def RLRedundancyScore(corpusList):\n",
        "    listOfSimilarityDoc=[]\n",
        "    for doc in corpusList:\n",
        "        sentences=sent_tokenize(doc)\n",
        "        docSimilarity=calculate_redundancy_Rouge_score(sentences)\n",
        "        listOfSimilarityDoc.append(docSimilarity)\n",
        "        #print(len(listOfSimilarityDoc))\n",
        "    ReScore=round(sum(listOfSimilarityDoc)/len(listOfSimilarityDoc),2)\n",
        "    return ReScore"
      ],
      "metadata": {
        "id": "rC5SAgP5JmG8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#N-Gram Ratio\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def calculate_unique_ngram_ratio(text, n):\n",
        "    # Tokenize the text using NLTK's word_tokenize\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Generate n-grams using NLTK's ngrams function\n",
        "    ngrams_list = list(ngrams(tokens, n))\n",
        "\n",
        "    if not ngrams_list:  # Handle the case where there are no n-grams\n",
        "        return 0.0\n",
        "\n",
        "    # Convert n-grams back to strings for counting\n",
        "    ngrams_strings = [' '.join(gram) for gram in ngrams_list]\n",
        "\n",
        "    # Calculate the counts of unique and total n-grams\n",
        "    unique_ngrams_count = len(set(ngrams_strings))\n",
        "    #print(unique_ngrams_count)\n",
        "    total_ngrams_count = len(ngrams_strings)\n",
        "    #print(total_ngrams_count)\n",
        "\n",
        "    # Calculate the unique n-gram ratio\n",
        "    unique_ngram_ratio = unique_ngrams_count / total_ngrams_count\n",
        "\n",
        "    return unique_ngram_ratio\n",
        "\n",
        "def NGramRedundancyScore(corpusList,n):\n",
        "    listOfSimilarityDoc=[]\n",
        "    for doc in corpusList:\n",
        "        docSimilarity=calculate_unique_ngram_ratio(doc,n)\n",
        "        listOfSimilarityDoc.append(docSimilarity)\n",
        "        #print(len(listOfSimilarityDoc))\n",
        "    ReScore=round(sum(listOfSimilarityDoc)/len(listOfSimilarityDoc),2)\n",
        "    return ReScore"
      ],
      "metadata": {
        "id": "6eP2h45uJ3rU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ReScore\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "\n",
        "def cosine(u, v):\n",
        "    return (np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v)))\n",
        "\n",
        "def redundancyMeasure(corpus):\n",
        "    sentences=sent_tokenize(corpus)\n",
        "    length=len(sentences)\n",
        "\n",
        "    #print(\"Sentence Count:\"+ str(len(sentences)))\n",
        "    if(len(sentences)<2):\n",
        "      return 0\n",
        "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "    sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "    sentenceGraph =np.zeros((length, length))\n",
        "    for x in range(length):\n",
        "        for y in range(length):\n",
        "            similarity= cosine(sentence_embeddings[x],sentence_embeddings[y])\n",
        "            sentenceGraph[x][y]=abs(similarity)\n",
        "    sentenceGraph = np.round (sentenceGraph,decimals=2)\n",
        "    #print(sentenceGraph)\n",
        "\n",
        "    return mean_exclude_diagonal(sentenceGraph)\n",
        "\n",
        "def mean_exclude_diagonal(matrix):\n",
        "    np.fill_diagonal(matrix, np.nan)\n",
        "    mean_value = np.nanmean(matrix)\n",
        "    return np.round (mean_value,decimals=2)\n",
        "\n",
        "def ReScore(corpusList):\n",
        "    listOfSimilarityDoc=[]\n",
        "    count=0\n",
        "    for doc in corpusList:\n",
        "        docSimilarity=redundancyMeasure(doc)\n",
        "        listOfSimilarityDoc.append(docSimilarity)\n",
        "        #print(len(listOfSimilarityDoc))\n",
        "        count=count+1\n",
        "        print(count)\n",
        "    ReScore=round(sum(listOfSimilarityDoc)/len(listOfSimilarityDoc),2)\n",
        "    return ReScore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzauvFrMKHvD",
        "outputId": "e1e5f82e-a177-496d-d49a-5f53c6f336de"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "# Define the file path\n",
        "file_path = \"/content/drive/Othercomputers/My Laptop/DriveAccess/OnlySimpleCTextRank-PubMed-Full.json\"  # Replace with the desired file path\n",
        "#file_path = \"/content/drive/Othercomputers/My Laptop/DriveAccess/OnlySimpleClustering-PubMed-Full.json\"\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    data_list = json.load(file)\n",
        "\n",
        "print(len(data_list[\"all_summary\"]))\n",
        "\n",
        "listOfDocuments=data_list[\"all_summary\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdH4fukbK2P0",
        "outputId": "b68b8103-09c4-494f-b721-40d47f16ee96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''print(\"###############################################################################\")\n",
        "print(\"unigram\")\n",
        "print(NGramRedundancyScore(listOfDocuments,1))\n",
        "print(\"bigram\")\n",
        "print(NGramRedundancyScore(listOfDocuments,2))\n",
        "print(\"trigram\")\n",
        "print(NGramRedundancyScore(listOfDocuments,3))\n",
        "print(\"###############################################################################\")\n",
        "print(\"ReScore\")\n",
        "print(ReScore(listOfDocuments))\n",
        "print(\"###############################################################################\")'''\n",
        "print(\"RL Redundancy SCORE:\")\n",
        "print(RLRedundancyScore(listOfDocuments))\n",
        "print(\"###############################################################################\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa6Ah00SKf9c",
        "outputId": "badd4d2f-eedd-471d-d148-91d4a991bcaf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RL Redundancy SCORE:\n",
            "0.2\n",
            "###############################################################################\n"
          ]
        }
      ]
    }
  ]
}