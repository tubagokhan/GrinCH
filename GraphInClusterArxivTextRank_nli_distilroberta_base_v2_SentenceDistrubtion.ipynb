{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tubagokhan/GrinCH/blob/main/GraphInClusterArxivTextRank_nli_distilroberta_base_v2_SentenceDistrubtion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc7RLp5LmBso",
        "outputId": "e0184c00-2fcc-476e-d1c1-2dc29dfda2bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThO6v86y2WAs",
        "outputId": "e2bc87e7-cfd0-462b-8f6a-4c188c08177c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: py-rouge==1.1 in /usr/local/lib/python3.10/dist-packages (1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install sentence_transformers\n",
        "!pip install py-rouge==1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZfgFyRF3kb8",
        "outputId": "633eeee1-2d5a-41c0-828f-eeb11415c618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk import sent_tokenize,word_tokenize\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import re\n",
        "\n",
        "from datasets import load_dataset\n",
        "from scipy.spatial import distance\n",
        "\n",
        "import math\n",
        "from math import*\n",
        "\n",
        "import rouge\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "import time\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "EETObHuc9vy8"
      },
      "outputs": [],
      "source": [
        "# Preprocessing method\n",
        "def preprocess_corpus(text):\n",
        "    # Remove special characters and extra whitespaces\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s.]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "def textSentenceCount(Text):\n",
        "    number_of_sentences = sent_tokenize(Text)\n",
        "    count=(len(number_of_sentences))\n",
        "    return count\n",
        "\n",
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "\n",
        "def createSummaryUsingKMeans(corpus, modelName):\n",
        "    sentences = sent_tokenize(corpus)\n",
        "    model = SentenceTransformer(modelName)\n",
        "    sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "    if len(sentences) >1000:\n",
        "      optimum_clusters = find_optimum_clusters(sentence_embeddings, 100)\n",
        "      print(\"Optimum cluster number:\", optimum_clusters)\n",
        "    else:\n",
        "      optimum_clusters = find_optimum_clusters(sentence_embeddings, int(len(sentences) / 3))\n",
        "      print(\"Optimum cluster number:\", optimum_clusters)\n",
        "\n",
        "    # Perform kmean clustering\n",
        "    kmeans = KMeans(n_clusters=optimum_clusters, random_state=0, n_init='auto').fit(sentence_embeddings)\n",
        "\n",
        "    chosen_sentence_indexes=[]\n",
        "    cluster_rank=[]\n",
        "    for cluster_id in range(optimum_clusters):\n",
        "        cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
        "        cluster_weight=clusterWeight(sentence_embeddings, cluster_indices)\n",
        "        cluster_rank.append(cluster_weight)\n",
        "        chosen_sentence_index=text_rank(sentence_embeddings, cluster_indices)\n",
        "        chosen_sentence_indexes.append(chosen_sentence_index)\n",
        "\n",
        "    if optimum_clusters>9:\n",
        "      choosen_clusters=k_highest_indices(cluster_rank, 9)\n",
        "      chosen_sentence_indexes = [chosen_sentence_indexes[i] for i in choosen_clusters]\n",
        "\n",
        "    sorted_indexes=sorted(chosen_sentence_indexes)\n",
        "    saveSummariesinJsonFile(len(sentences), sorted_indexes,file_Path_Sent_Dist)\n",
        "\n",
        "    chosen_sentences = []\n",
        "    for chosen_sentence_index in sorted_indexes:\n",
        "        chosen_sentences.append(sentences[chosen_sentence_index])\n",
        "\n",
        "    summary = \" \".join(chosen_sentences)\n",
        "\n",
        "    return summary\n",
        "\n",
        "def convert_int64_to_int(obj):\n",
        "    if isinstance(obj, int):\n",
        "        return int(obj)\n",
        "    raise TypeError\n",
        "\n",
        "def saveSummariesinJsonFile(num_sents, sorted_indexes,file_Path_Sent_Dist):\n",
        "    filePath=file_Path_Sent_Dist\n",
        "    # Create a dictionary with the required data structure\n",
        "    data = {\n",
        "        \"num_sents\": num_sents,\n",
        "        \"summary\": [[int(index)] for index in sorted_indexes]\n",
        "    }\n",
        "\n",
        "    # Check if the file path exists\n",
        "    if os.path.exists(filePath):\n",
        "        try:\n",
        "            # If the file exists and is not empty, read the existing data\n",
        "            with open(filePath, \"r\") as file:\n",
        "                existing_data = json.load(file)\n",
        "        except json.JSONDecodeError:\n",
        "            # If the file exists but contains invalid JSON data, initialize with an empty list\n",
        "            existing_data = []\n",
        "    else:\n",
        "        # If the file does not exist, create a new file with the data\n",
        "        existing_data = []\n",
        "\n",
        "    # Append the new data to the existing data\n",
        "    existing_data.append(data)\n",
        "\n",
        "    # Write the updated data to the file\n",
        "    with open(filePath, \"w\") as file:\n",
        "        json.dump(existing_data, file, indent=2, default=convert_int64_to_int)\n",
        "\n",
        "#function calculates the optimal number of clusters using the elbow method. The function plots the elbow curve, which shows the inertia values for different cluster numbers. The user can visually inspect the plot to determine the elbow point, indicating the optimal number of clusters.\n",
        "def find_optimum_clusters(data, max_clusters):\n",
        "    inertias = []\n",
        "    for k in range(1, max_clusters + 1):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=0).fit(data)\n",
        "        inertias.append(kmeans.inertia_)\n",
        "\n",
        "    # Plotting the elbow curve\n",
        "    #plt.plot(range(1, max_clusters + 1), inertias)\n",
        "    #plt.xlabel(\"Number of Clusters\")\n",
        "    #plt.ylabel(\"Inertia\")\n",
        "    #plt.title(\"Elbow Curve\")\n",
        "    #plt.show()\n",
        "\n",
        "    # Calculate the optimal number of clusters using the elbow method\n",
        "    diff = np.diff(inertias)\n",
        "    acceleration = np.diff(diff)\n",
        "    opt_cluster_num = acceleration.argmin() + 2  # Adding 2 to get the index of the minimum acceleration\n",
        "    return opt_cluster_num\n",
        "\n",
        "def clusterWeight(sentence_embeddings, cluster_indices):\n",
        "  length=len(cluster_indices)\n",
        "  sentenceGraph =np.zeros((length, length))\n",
        "  for x in range(length):\n",
        "    for y in range(length):\n",
        "      if x>y:\n",
        "        similarity= cosine(sentence_embeddings[cluster_indices[x]],sentence_embeddings[cluster_indices[y]])\n",
        "        sentenceGraph[x][y]=abs(similarity)\n",
        "    #print(sentenceGraph)\n",
        "  SumElement=(np.concatenate(sentenceGraph).sum())\n",
        "  return round(SumElement,2)\n",
        "\n",
        "def k_highest_indices(cluster_weight_list, k):\n",
        "    # Enumerate the list to keep track of original indices\n",
        "    enumerated_list = list(enumerate(cluster_weight_list))\n",
        "\n",
        "    # Sort the enumerated list in descending order based on the float values\n",
        "    sorted_list = sorted(enumerated_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Extract the indices of the k highest elements\n",
        "    k_highest_indices = [item[0] for item in sorted_list[:k]]\n",
        "\n",
        "    return k_highest_indices\n",
        "\n",
        "def text_rank(sentence_embeddings, indexes):\n",
        "    # Filter embeddings based on the input indexes\n",
        "    filtered_embeddings = np.array([sentence_embeddings[i] for i in indexes])\n",
        "\n",
        "    # Calculate cosine similarity between filtered sentence embeddings\n",
        "    similarity_matrix = cosine_similarity(filtered_embeddings, filtered_embeddings)\n",
        "\n",
        "    # Create a graph using similarity matrix\n",
        "    graph = nx.from_numpy_array(similarity_matrix)\n",
        "    try:\n",
        "      # Apply TextRank algorithm to rank the sentences\n",
        "      scores = nx.pagerank(graph, max_iter=1000)\n",
        "          # Get the index of the highest-ranked sentence\n",
        "      highest_ranked_index = max(scores, key=scores.get)\n",
        "\n",
        "      # Get the original index from the list of indexes\n",
        "      highest_ranked_original_index = indexes[highest_ranked_index]\n",
        "      return highest_ranked_original_index\n",
        "    except:\n",
        "      print(\"######### Error with PageRank\")\n",
        "      return indexes[0]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jYrBbXjzI8y8"
      },
      "outputs": [],
      "source": [
        "def save_data_to_json(file_path, summary, goldstandard, document):\n",
        "    try:\n",
        "        data = {\n",
        "            \"all_summary\": [],\n",
        "            \"all_goldstandard\": [],\n",
        "            \"all_document\": []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r') as file:\n",
        "                data = json.load(file)\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "        all_summary = data.get('all_summary', [])\n",
        "        all_goldstandard = data.get('all_goldstandard', [])\n",
        "        all_document = data.get('all_document', [])\n",
        "\n",
        "        all_summary.append(summary)\n",
        "        all_goldstandard.append(goldstandard)\n",
        "        all_document.append(document)\n",
        "\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            json.dump(data, file)\n",
        "\n",
        "        print(\"Data saved successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrKrlYxN3auN",
        "outputId": "084246fb-1e18-452b-b162-8eb5fcfb9d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document: 3246\n",
            "Document sentence number: 3461\n",
            "Optimum cluster number: 87\n",
            "Summary sentence number: 9\n",
            "Data saved successfully.\n",
            "Document processing time: 14:31\n",
            "Total processing time: 01:00:14:31\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "#modelName = 'all-mpnet-base-v2'\n",
        "modelName ='nli-distilroberta-base-v2'\n",
        "dataset = load_dataset(\"scientific_papers\", \"arxiv\")\n",
        "datasetName='-Arxiv'\n",
        "\n",
        "path='/content/drive/Othercomputers/My Laptop/DriveAccess/'\n",
        "\n",
        "# Define the file path\n",
        "file_path = path+\"GrinchAllGoldAndMySummaries-\"+modelName+datasetName+\".json\"  # My Summaries, Gold Summaires, Documents\n",
        "file_Path_Sent_Dist=path+\"GrinchSentenceDistrubtionsOfSummaries-\"+modelName+datasetName+\".json\"  # Sentence Distrubition Summaries, Summary Sentence Indexes\n",
        "\n",
        "\n",
        "startTimeforOverall = time.time()\n",
        "# document size 6440\n",
        "# 3245, 3595 have problem iinclude 3500 sentneces\n",
        "N = 1 # HOW MANY DOCUMENT MALCOLLLMMMMMMM\n",
        "startN = 3245 # Which document we will start\n",
        "\n",
        "for d in range(N):\n",
        "    startTimeforDocument = time.time()\n",
        "    print(\"Document:\", startN + d + 1)\n",
        "    corpus = dataset['test']['article'][startN + d]\n",
        "    corpus = preprocess_corpus(corpus)\n",
        "    print(\"Document sentence number:\", textSentenceCount(corpus))\n",
        "\n",
        "    summary=\"\"\n",
        "    if textSentenceCount(corpus) > 8:\n",
        "        summary = createSummaryUsingKMeans(corpus, modelName)\n",
        "        print(\"Summary sentence number:\", textSentenceCount(summary))\n",
        "    else:\n",
        "      summary=corpus\n",
        "      print(\"Corpus is less than 9 sentence, summarization didn't apply sentence number:\", textSentenceCount(corpus))\n",
        "\n",
        "\n",
        "    save_data_to_json(file_path, summary, dataset['test']['abstract'][startN + d], dataset['test']['article'][startN + d])\n",
        "\n",
        "    elapsedTimeforDocument = time.time() - startTimeforDocument\n",
        "    elapsedTimeforAll = time.time() - startTimeforOverall\n",
        "    print('Document processing time: '+time.strftime(\"%M:%S\", time.gmtime(elapsedTimeforDocument)))\n",
        "    print('Total processing time: '+time.strftime(\"%d:%H:%M:%S\", time.gmtime(elapsedTimeforAll)))\n",
        "\n",
        "    print(\"----------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "po9dIR53OKQ-"
      },
      "outputs": [],
      "source": [
        "def prepare_results(m, p, r, f):\n",
        "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(m, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
        "\n",
        "def rougeEvaluation(all_hypothesis, all_references):\n",
        "\n",
        "    for aggregator in ['Avg']:\n",
        "        print('Evaluation with {}'.format(aggregator))\n",
        "        apply_avg = aggregator == 'Avg'\n",
        "        apply_best = aggregator == 'Best'\n",
        "\n",
        "        evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
        "                               max_n=4,\n",
        "                               limit_length=False,\n",
        "                               length_limit=1000,\n",
        "                               length_limit_type='words',\n",
        "                               apply_avg=apply_avg,\n",
        "                               apply_best=apply_best,\n",
        "                               alpha=0.2, # Default F1_score\n",
        "                               weight_factor=1.2,\n",
        "                               stemming=True)\n",
        "\n",
        "        scores = evaluator.get_scores(all_hypothesis, all_references)\n",
        "\n",
        "        for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
        "            if not apply_avg and not apply_best: # value is a type of list as we evaluate each summary vs each reference\n",
        "                for hypothesis_id, results_per_ref in enumerate(results):\n",
        "                    nb_references = len(results_per_ref['p'])\n",
        "                    for reference_id in range(nb_references):\n",
        "                        print('\\tHypothesis #{} & Reference #{}: '.format(hypothesis_id, reference_id))\n",
        "                        print('\\t' + prepare_results(metric,results_per_ref['p'][reference_id], results_per_ref['r'][reference_id], results_per_ref['f'][reference_id]))\n",
        "                print()\n",
        "            else:\n",
        "                print(prepare_results(metric, results['p'], results['r'], results['f']))\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "oCxJykpEXFVp"
      },
      "outputs": [],
      "source": [
        "file_path=\"/content/drive/Othercomputers/My Laptop/DriveAccess/GrinchAllGoldAndMySummaries-nli-distilroberta-base-v2-Arxiv.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuS9XFy_Lpid",
        "outputId": "312f4985-333b-4489-c2ef-c9b27432c65f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data read successfully.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "    mysummaries = data.get('all_summary', [])\n",
        "    goldstandardsummaries = data.get('all_goldstandard', [])\n",
        "    documents = data.get('all_document', [])\n",
        "\n",
        "    print(\"Data read successfully.\")\n",
        "except FileNotFoundError:\n",
        "  print(f\"Error: File '{file_path}' not found.\")\n",
        "  mysummaries, goldstandardsummaries, documents  = []\n",
        "except Exception as e:\n",
        "  print(f\"Error: {e}\")\n",
        "  mysummaries, goldstandardsummaries, documents  = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e_sOzFSGK2Y",
        "outputId": "5e6f66fa-7f6d-4edb-e356-bae856544a4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6440"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNRr0kl5eHZM",
        "outputId": "a3fb636a-d096-4c7a-d1d1-e440060e6e58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6440"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "dataset = load_dataset(\"scientific_papers\", \"arxiv\")\n",
        "len(dataset['test']['article'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBXQ0G2-PnlW",
        "outputId": "f8867735-a74b-4e70-bba3-5a112521fe4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation with Avg\n",
            "\trouge-1:\tP: 29.37\tR: 51.05\tF1: 42.70\n",
            "\trouge-2:\tP:  8.49\tR: 14.90\tF1: 12.41\n",
            "\trouge-3:\tP:  3.07\tR:  5.37\tF1:  4.47\n",
            "\trouge-4:\tP:  1.47\tR:  2.55\tF1:  2.13\n",
            "\trouge-l:\tP: 28.02\tR: 44.36\tF1: 38.62\n",
            "\trouge-w:\tP: 12.94\tR: 11.59\tF1: 11.44\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rougeEvaluation(mysummaries, goldstandardsummaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlASXlODYurY",
        "outputId": "6ca876b8-94aa-41ed-ea22-025958609352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "def find_duplicates_indexes(lst):\n",
        "    duplicates = {}\n",
        "    for i, item in enumerate(lst):\n",
        "        if item in duplicates:\n",
        "            duplicates[item].append(i)\n",
        "        else:\n",
        "            duplicates[item] = [i]\n",
        "\n",
        "    # Extract index lists for duplicate values\n",
        "    duplicate_indexes = [indexes for indexes in duplicates.values() if len(indexes) > 1]\n",
        "\n",
        "    # Flatten the list of index lists\n",
        "    flattened_indexes = [index for indexes in duplicate_indexes for index in indexes]\n",
        "\n",
        "    return flattened_indexes\n",
        "\n",
        "\n",
        "\n",
        "result = find_duplicates_indexes(goldstandardsummaries)\n",
        "print(result)\n",
        "\n",
        "result = find_duplicates_indexes(documents)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6oZdYJJf8bt",
        "outputId": "202a1e53-4cda-474e-f0d3-91aae507eb8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "def find_missing_values_indexes(list1, list2):\n",
        "    missing_indexes = [index for index, value in enumerate(list1) if value not in list2]\n",
        "    return missing_indexes\n",
        "\n",
        "# Example usage:\n",
        "list1 = dataset['test']['article']\n",
        "list2 = documents\n",
        "missing_indexes = find_missing_values_indexes(list1, list2)\n",
        "print(missing_indexes)\n",
        "len(missing_indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugINSfHKM7rU",
        "outputId": "37c49b5f-5bfb-4d3e-a99d-16a8675b066e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.755124223602484"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "sentencenumber=0\n",
        "for i in range(len(mysummaries)):\n",
        "  sentencenumber=sentencenumber+textSentenceCount(mysummaries[i])\n",
        "avg=sentencenumber/len(mysummaries)\n",
        "avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irx5-_a0M_4G",
        "outputId": "b2981e1a-8606-4575-f544-7f7ac14f108d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.226708074534161"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "sentencenumber=0\n",
        "for i in range(len(goldstandardsummaries)):\n",
        "  sentencenumber=sentencenumber+textSentenceCount(goldstandardsummaries[i])\n",
        "avg=sentencenumber/len(goldstandardsummaries)\n",
        "avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "q7CA0TIIXmxP"
      },
      "outputs": [],
      "source": [
        "# Play an audio beep. Any audio URL will do.\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "gU5V1SZ65exd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "664aecf4-534e-4a6a-9b80-5f79af16e53f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6440\n"
          ]
        }
      ],
      "source": [
        "print(len(mysummaries))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "rKNx-Zzk0CoT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4fcea4d6-9a3f-4712-c535-44640ce23074"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for i in range (1):\\n  duplicatedItemIndex=5437\\n  del mysummaries[duplicatedItemIndex]\\n  del goldstandardsummaries[duplicatedItemIndex]\\n  del documents[duplicatedItemIndex]\\n\\nresult = find_duplicates_indexes(goldstandardsummaries)\\nprint(result)\\n\\nresult = find_duplicates_indexes(documents)\\nprint(result)\\nprint(len(result))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "'''for i in range (1):\n",
        "  duplicatedItemIndex=5437\n",
        "  del mysummaries[duplicatedItemIndex]\n",
        "  del goldstandardsummaries[duplicatedItemIndex]\n",
        "  del documents[duplicatedItemIndex]\n",
        "\n",
        "result = find_duplicates_indexes(goldstandardsummaries)\n",
        "print(result)\n",
        "\n",
        "result = find_duplicates_indexes(documents)\n",
        "print(result)\n",
        "print(len(result))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "C0pHlovd19Qu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "4AhPip9m0I6q"
      },
      "outputs": [],
      "source": [
        "#file_path_new=\"/content/drive/Othercomputers/My Laptop/DriveAccess/GrinchAllGoldAndMySummaries-nli-distilroberta-base-v2-Arxiv2.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "BgTtwjeR0RdV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aa6d3517-cb73-4ad6-88ae-dbd35e68eccb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data = {\\n  \"all_summary\": mysummaries,\\n  \"all_goldstandard\": goldstandardsummaries,\\n  \"all_document\": documents\\n}\\n\\nwith open(file_path_new, \"w\") as json_file:\\n  json.dump(data, json_file)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "'''data = {\n",
        "  \"all_summary\": mysummaries,\n",
        "  \"all_goldstandard\": goldstandardsummaries,\n",
        "  \"all_document\": documents\n",
        "}\n",
        "\n",
        "with open(file_path_new, \"w\") as json_file:\n",
        "  json.dump(data, json_file)'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1DtcgcWiDV9DXRItDHRcfBBIOrGMCnfOO",
      "authorship_tag": "ABX9TyN2tXT7FzhoyBfJswc5Cac/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}